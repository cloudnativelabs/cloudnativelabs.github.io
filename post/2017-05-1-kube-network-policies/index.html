<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>K8s Network Policies with iptables</title>
  <meta property="og:title" content="K8s Network Policies with iptables" />
  <meta name="twitter:title" content="K8s Network Policies with iptables" />
  <meta name="description" content="Network policies in Kubernetes provides primary means to secure a pod by exerting control over who can connect to pod. Intent of this blog post is not to describe what network policies are but to show how iptables on the the cluster nodes can be used to build a distributed firewall solution that enforces network policies in Kubernetes clusters. This write up draws up from the insights of implementing a network policy controller in Kube-router.">
  <meta property="og:description" content="Network policies in Kubernetes provides primary means to secure a pod by exerting control over who can connect to pod. Intent of this blog post is not to describe what network policies are but to show how iptables on the the cluster nodes can be used to build a distributed firewall solution that enforces network policies in Kubernetes clusters. This write up draws up from the insights of implementing a network policy controller in Kube-router.">
  <meta name="twitter:description" content="Network policies in Kubernetes provides primary means to secure a pod by exerting control over who can connect to pod. Intent of this blog post is not to describe what network policies are but to show â€¦">
  <meta name="author" content="Cloudnative Labs"/>
  <meta name="twitter:card" content="summary" />
  <meta property="og:url" content="https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="" />

  <meta name="generator" content="Hugo 0.19" />
  <link rel="canonical" href="https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/" />
  <link rel="alternate" href="https://cloudnativelabs.github.io/index.xml" type="application/rss+xml" title="">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cloudnativelabs.github.io/css/main.css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://cloudnativelabs.github.io/css/pygment_highlights.css" />
  <link rel="stylesheet" href="https://cloudnativelabs.github.io/css/highlight.min.css" />


</head>

  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://cloudnativelabs.github.io/"></a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/">Blog</a>
            </li>
          
        

        

        
      </ul>
    </div>

    <div class="avatar-container">
      <div class="avatar-img-border">
        
      </div>
    </div>

  </div>
</nav>




    
  
  
  




  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              <h1>K8s Network Policies with iptables</h1>
                
                
                  <span class="post-meta">
  Posted on May 3, 2017
  
</span>


                
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        

<p>Network policies in Kubernetes provides primary means to secure a pod by exerting control over who can connect to pod. Intent of this blog post is not to describe what network policies are but to show how iptables on the the cluster nodes can be used to build a distributed firewall solution that enforces network policies in Kubernetes clusters. This write up draws up from the insights of implementing a network policy controller in <a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a>. But concepts described can be used to build your own version of network policy enforcer with iptables.</p>

<p>Kubernetes networking has following security model:</p>

<ul>
<li>for every pod by default ingress is allowed, so a pod can receive traffic from any one</li>
<li>default allow behaviour can be changed to default deny on per namespace basis. When a namespace is configured with isolation tye of <strong>DefaultDeny</strong> no traffic is allowed to the pods in that namespace</li>
<li>when a namespace is configured with <strong>DefaultDeny</strong> isolation type, network policies can be configured in the namespace to whitelist the traffic to the pods in that namespace</li>
</ul>

<p>Lets quickly recap what network policies are then we will dive in to the iptables based solution aspects.</p>

<h3 id="network-policy">network policy</h3>

<p>Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDR or IP used for matching source or destination IP&rsquo;s.
Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of app) and select subsets of objects. A typical network policy looks like below:</p>

<pre><code>apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
   - from:
     - namespaceSelector:
       matchLabels:
         project: myproject
     - podSelector:
       matchLabels:
       role: frontend
   ports:
     - protocol: tcp
     port: 6379
</code></pre>

<p>In the network policy <strong>matchLabels</strong> in  <strong>podSelector</strong> section is used to identify set of destination pods for which a network policy applies. Similarly ingress section of network policy has a <strong>matchLabels</strong> in <strong>podSelector</strong> section which identifies the set of pods that will become source pods set. Alternativley <strong>namespaceSelector</strong> in the ingress section can be used to select all pods in a namespace. Ports section of network policy spec defines one or more port and protocol combinations that are to be opened up.</p>

<h3 id="evaluating-networking-policies">evaluating networking policies</h3>

<p>Network policies are declarative in nature to express application intent. So its implementation of network policies responsibility to translate the intent to configuration. Kubernetes provides a convenient way to retrieve the set of pods matching a label selector through API or kubectl as shown below.</p>

<pre><code>curl 192.168.1.99:8080/api/v1/namespaces/testns1/pods?labelSelector=app%3Dguestbook,tier%3Dfrontend
kubectl get pods -o json --selector=app=guestbook,tier=frontend --namespace=testns1 | jq '.items[] | .status | .podIP'
</code></pre>

<p>So its easy to get the set of source pod IP&rsquo;s and set of destination pod IP&rsquo;s based on the <strong>matchLables</strong> in network policy spec. Essentially a network policy implicitly evaluates to be definition of set of source pods that can talk to set of destination pod over specified port and protocol. Pods being ephermal in nature, set of pods matching the label selector is a dynamic set. In later section we will see how this poses challenges in how delta changes to iptable configuration applied with minimal disruption to datapath. But first lets see how we can configure iptables at a given time to reflect current state of network policies.</p>

<h3 id="representing-network-policy-in-iptables">representing network policy in iptables</h3>

<p>Since we are only concerned about the filtering of the packets, we will use only iptable filter functionality and all the examples below refer to use of <strong>filter</strong> table.</p>

<p>We have seen how we can evaluate a network policy to a whitelist rules expressing set of source pod IP&rsquo;s that can reach port and protocol on set of destination pod IP&rsquo;s. A natural choice is to represent each network policy as user defined chain. A network policy evaluates to single destination pod set. But a network policy specification can have multiple ingress rules. Each ingress rule evaluates to set of pods that are source pods, port and protocol combination. Below is pseudo code how user chain for the network policy can be populated with rules to accept the traffic.</p>

<pre><code>for each dstIP in set of destination pod IP as per the podSelector in the network policy {
    for each ingressRule in ingressRules of network policy {
        for srcIP in set of source pod IP as per the podSelector in ingress rule of the network policy {
            for port, protocol in ports of the network policy {
                 add rule '-A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT' 
            }
        }
    }
}
</code></pre>

<p>Since we have a rule for each source pod ip and destination pod IP, as you may have suspected this will blow up the chain with large number of rules. Fortunatley we can use <strong>ipset</strong> to better represent the rules and more importantly data path can scale well. A slightly modified pseudo code to populate the rules in the chain.</p>

<pre><code>form ipset dstIpset with all destination pod ip evaluated as per the podSelector in the policy
for each ingressRule in ingressRules of network policy {
    form ipset srcIpset wth all source pod IP as evaluated as per the podSelector in ingress rule of the network policy {
    for port, protocol in ports of the network policy {
         add rule '-A KUBE-NWPLCY-CHAIN -p $protocol -m set --match-set $srcIpset src -m set --match-set dstIpset dst -m $protocol --dport $port -j ACCEPT'
    }
}
</code></pre>

<p>Each cluster node can configured with representation of network policies with iptables as above. Destination ipset can be further optimized to represent pod IP of the pods running on specific node. Now lets see how inbound traffic to pod can be run through the network policy chains.</p>

<h3 id="pod-ingress-traffic-paths">Pod ingress traffic paths</h3>

<p>There are multiple data paths through which packet is received by a pod as shown in below diagram.</p>

<p><img src="/img/pod-ingress-traffic.jpg" alt="Pod Ingress traffic" /></p>

<ol>
<li>Pod on a node reaching to pod on same node through bridge (pod2-&gt;pod3)</li>
<li>Pod on a node reaching to pod on same node but through the service proxy (pod1-&gt;pod2)</li>
<li>Pod on a node reaching to pod on other node (pod3-&gt;pod4)</li>
<li>Pod on a node reaching to pod on other node through service proxy (pod5-&gt;pod6)</li>
<li>external client reaching to pod (external client-&gt;pod7)</li>
</ol>

<h3 id="pod-specific-firewall-chain">pod specific firewall chain</h3>

<p>Ingress traffic to a pod can be whitelisted through one or more network policies. So its important that packet destined to a pod is run through the networssary  chains. Natural choice would be to have a iptable chain per pod. Now we need to run through the traffic destined for a pod through pod firewall chain. We only have to consider the pods whose namespace has ingress set to DefaultDeny configuration. Also on a cluster node, we are only concerned about the pods running on the node.</p>

<p>For the case #1 data path above we will need to use <strong>physdev</strong> module to match the traffic that is getting bridged locally. From iptables perspective it will still go through the FORWARD chain. For the case #2 from iptables perspective it is output packer, so it will only hit OUTPUT chain. Rest of the cases #3 to #5 all fall under the same category. Traffic will hit the FORWARD chain. Following is the pseudo code to populate FORWARD and OUTPUT chains to jump the traffic destined for a pod to pod specific firewall chain.</p>

<pre><code>for each pod running on the node {
    if pod namespace has 'DefautDeny' as ingress type {
        add rule '-A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN'
        add rule '-A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN'
        add rule '-A OUTPUT -d $podIP  -j KUBE-POD-SPECIFIC-FW-CHAIN'
    }
}
</code></pre>

<h3 id="rules-to-run-through-network-policies">rules to run through network policies</h3>

<p>We will need rules in the pod specific firewall chain to take appropriate actions. First we have default rule to REJECT the traffic in case traffic destined to pod does not match any white list rules in the network policy chains. We will need a rule to permit the return traffic to the pod (for the connections which originated from the pod) using conntrack. A sample set of rules in the pod specific firewall chain would look like below. Number of rules to jump to the network policy chains depends on the how many network policies are applicable to a pod.</p>

<pre><code>-A KUBE-POD-SPECIFIC-FW-CHAIN -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN1
-A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN2
-A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN3
-A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable
</code></pre>

<h3 id="state-synchronization">state synchronization</h3>

<p>Like mentioned earlier network policies only express the intent. So its implementations responsibility to translate the intent to desired configuration. There are number of events that will impact the iptables configuration. For e.g.</p>

<ul>
<li>add/delete of pods</li>
<li>modification to namespace default ingress behaviour</li>
<li>add/delete of network policies</li>
</ul>

<p>Implementation of network policies have to watch for the events from Kubernetes API server and have to reflect the changes in the iptables configuration.</p>

<h3 id="conclusion">conclusion</h3>

<p>We went over how iptables can used to implement a solution that can enforce network policies on each cluster node. An implementation of approach discussed in the article is implemented in <a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a>. You can deploy Kube-router and see how iptables are used as effective solution to enforce network policies.</p>

<p><div class="github-card" data-github="cloudnativelabs/kube-router" data-width="730" data-height="" data-theme="default"></div>
<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script></p>

      </article>

      <ul class="pager blog-pager">
        
          <li class="previous">
            <a href="https://cloudnativelabs.github.io/post/2017-04-19-kube-router/" data-toggle="tooltip" data-placement="top" title="Kube-router">&larr; Previous Post</a>
          </li>
        
        
      </ul>

      
        
          <div class="disqus-comments">
            <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'cloudnativelabs';
    var disqus_identifier = 'https:\/\/cloudnativelabs.github.io\/post\/2017-05-1-kube-network-policies\/';
    var disqus_title = 'K8s Network Policies with iptables';
    var disqus_url = 'https:\/\/cloudnativelabs.github.io\/post\/2017-05-1-kube-network-policies\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
          </div>
        
      

    </div>
  </div>
</div>

    <footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="https://github.com/cloudnativelabs" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
        </ul>
        <p class="credits copyright text-muted">
          Cloudnative Labs
          &nbsp;&bull;&nbsp;
          2017

          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="http://gohugo.io">Hugo v0.19</a> powered &nbsp;&bull;&nbsp; Theme by <a href="http://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a> adapted to <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a>
          
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha384-dq1/gEHSxPZQ7DdrM82ID4YVol9BYyU7GbWlIwnwyPzotpoc57wDw/guX8EaYGPx" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="https://cloudnativelabs.github.io/js/main.js"></script>
<script src="https://cloudnativelabs.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> renderMathInElement(document.body); </script>






<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-97817717-1', 'auto');
ga('send', 'pageview');
</script>



  </body>
</html>

