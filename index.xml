<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://cloudnativelabs.github.io/index.xml</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>info@cloudnativelabs.net (Cloudnative Labs)</managingEditor>
    <webMaster>info@cloudnativelabs.net (Cloudnative Labs)</webMaster>
    <lastBuildDate>Fri, 12 May 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://cloudnativelabs.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kubernetes network services proxy with IPVS/LVS</title>
      <link>https://cloudnativelabs.github.io/post/2017-05-10-kube-network-service-proxy/</link>
      <pubDate>Fri, 12 May 2017 00:00:00 +0000</pubDate>
      <author>info@cloudnativelabs.net (Cloudnative Labs)</author>
      <guid>https://cloudnativelabs.github.io/post/2017-05-10-kube-network-service-proxy/</guid>
      <description>

&lt;p&gt;A Kubernetes Service is an abstraction which groups a logical set of Pods that provide the same functionality. A service in Kubernetes can be of different types, of which &amp;lsquo;ClusterIP&amp;rsquo; and &amp;lsquo;NodePort&amp;rsquo; types forms basis for service discovery and load balancing. Both of the service types requires a service proxy running on each of the cluster node. Kubernetes has an implementation of service proxy &amp;lsquo;Kube-proxy&amp;rsquo; based on iptables. While Kube-proxy provides out-of-box solution its not neccessarily an optimal solution for all users.
In this blog we will go over an implementation of Kubernetes network services proxy built on top of Linux IPVS in &lt;a href=&#34;https://github.com/cloudnativelabs/kube-router&#34;&gt;Kube-router&lt;/a&gt;.
We will also go over pros and cons of Kube-proxy which is based on iptables and compare it aginest in general any implemenation of IPVS/LVS based network service proxy for Kubernetes.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr&#34;&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;Please see the demo to get the feel for IPVS in action as service proxy for Kubernetes as implemented in Kube-router.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/120312&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/120312.png&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lets delve into the details now.&lt;/p&gt;

&lt;h3 id=&#34;clusterip-and-nodeport-services&#34;&gt;ClusterIP and NodePort services&lt;/h3&gt;

&lt;p&gt;Each Service of type &amp;lsquo;ClusterIP&amp;rsquo; and &amp;lsquo;NodePort&amp;rsquo;is assigned a unique IP address (called as clusterIP) throught the lifespan of the Service. Pods can be configured to talk to the Service through cluster IP and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.
Additionally for the Service of &amp;ldquo;NodePort&amp;rdquo; type, the Kubernetes master will allocate a port from a flag-configured range (default: 30000-32767), and each Node will proxy that port (the same port number on every Node) into your Service.&lt;/p&gt;

&lt;h3 id=&#34;ipvs&#34;&gt;IPVS&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://kb.linuxvirtualserver.org/wiki/IPVS&#34;&gt;IPVS (IP Virtual Server)&lt;/a&gt; implements transport-layer (L4) load balancing inside the Linux kernel. IPVS running on a host acts as a load balancer at the front of a cluster of real servers, it can direct requests for TCP/UDP based services to the real servers, and makes services of the real servers to appear as a virtual service on a single IP address. IPVS support rich set of connection scheduling algorithms (Round-Robin, Weighted Round-Robin, Least-Connection etc.) inside the Linux kernel.
IPVS is known to be fast as its implemented in Kernel and provides different IP load balancing techniques (NAT, TUN and LVS/DR).&lt;/p&gt;

&lt;h3 id=&#34;kubernetes-service-proxy-with-ipvs&#34;&gt;Kubernetes service proxy with IPVS&lt;/h3&gt;

&lt;p&gt;As we learned earlier each of the Kubernetes service of type ClusterIP and NodePort needs L4 load balancer and service proxy on each of the cluster node. Kubernetes notion of Service and Endpoints logically can be mapped cleanly to IPVS virtual service and server notions. With IPVS doing heavy lifting its fairly easy to implement a soltion for Kubernetes that provides service proxy for ClusterIP and NodePort services. We just need a agent running on each of cluster node that watches
Kubernetes API server for changes to Services and Endpoints and reflect desired state in IPVS.&lt;/p&gt;

&lt;h3 id=&#34;kube-router-implementation-of-service-proxy-with-ipvs&#34;&gt;Kube-router implementation of service proxy with IPVS&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router&#34;&gt;Kube-router&lt;/a&gt; has an implementation of IPVS based service proxy for ClusterIP and NodePort services. Kube-router watches Kubernetes API server for changes (add/delete/updates) to Services and Endpoints objects and configures IPVS virtual service and servers. While mapping of Service and Endpoints to IPVS virtual service and server is stright forward, there are some challenges due to Kubernetes networking model.&lt;/p&gt;

&lt;h4 id=&#34;cluster-ip&#34;&gt;cluster ip&lt;/h4&gt;

&lt;p&gt;Typically Cluster IP&amp;rsquo;a are not routable (not a hard restriction from Kubernetes and one can hook up these IP in to the their non-cluster routing environment). A Cluster IP will be shared by all the nodes in the cluster. At the bare minimul the pods on the node need to be able to access the cluster IP. Kube-router solves the problem by having dummy interface on each node which will be exclusiveley used to assign cluster IP&amp;rsquo;s. Once IP is assigned to the node, pods on the node can
connect to the cluster IP&amp;rsquo;s. Traffic to cluster IP originating external to the nodes are not dropped by default.&lt;/p&gt;

&lt;h4 id=&#34;reverse-traffic&#34;&gt;reverse traffic&lt;/h4&gt;

&lt;p&gt;Kube-router uses IPVS NAT mode for load balancing technique. Like any load balancing mechanism based on DNAT, reverse path traffic need to go through the load balancer for end-to-end functioning. There are two data paths thats needs to meet this requirement. For the traffic originating from the pods on the node, only destanation NAT is performed. Since Kube-router uses host-based routing for pod-to-pod networking, revese path traffic goes through the same node from which traffic originated. Traffic originating external to
cluster and accessing node ports needs masqurading traffic so that SNAT is performed. On the reverse path, traffic goes through the same node as due to SNAT.&lt;/p&gt;

&lt;h4 id=&#34;ipvs-conntrack&#34;&gt;ipvs conntrack&lt;/h4&gt;

&lt;p&gt;IPVS uses its own simple and fast connection tracking for performance reasons, instead of using netfilter connection tracking. Kube-router enables IPVS connection tracking.&lt;/p&gt;

&lt;h3 id=&#34;kube-proxy&#34;&gt;Kube-proxy&lt;/h3&gt;

&lt;p&gt;Kube-proxy is a network proxy service that runs on each cluster node and proxies L4 sessions to service VIP (service cluster ip) or node port to the back end pods in round robin fashion. Kube-proxy is built on top of iptables functionality in Linux.&lt;/p&gt;

&lt;h4 id=&#34;pros&#34;&gt;pros&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;leverages iptables, so pretty much any functionality (SNAT, DNAT, port translation etc) can be achiveved. Maniputlate packet at different stages (pre-routing, post routing, input, forward, output etc) can be done&lt;/li&gt;
&lt;li&gt;supports port ranges&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;cons&#34;&gt;cons&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;arcane implementation of load balancer, hard to troubleshoot anything with out understanding the how Kube-proxy uses iptables to implement load balancer&lt;/li&gt;
&lt;li&gt;not a true load balancer but a simple round robin forwarder&lt;/li&gt;
&lt;li&gt;no load balancing mechanisms&lt;/li&gt;
&lt;li&gt;iptable performance degrades as number of services increases. More number of services means long list of iptable rules in a chain to match a packet aginest in the data path, and latency in insert/delete rules in control path&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ipvs-lvs-based-service-proxy&#34;&gt;IPVS/LVS based service proxy&lt;/h3&gt;

&lt;h4 id=&#34;pros-1&#34;&gt;pros&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;hash based matching vs list based iptables rules in the chains.&lt;/li&gt;
&lt;li&gt;biggest advantage is easy to verify configuration with ipvsadm&lt;/li&gt;
&lt;li&gt;been around for long time in linux kernel and widely used&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;cons-1&#34;&gt;cons&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;still need iptable tweeks to achive masqurading etc&lt;/li&gt;
&lt;li&gt;direct routing does not handle port remapping. So direct routing which is the fastest of load balancing algorithm can not be used&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;conclusion&lt;/h3&gt;

&lt;p&gt;An IPVS based service proxy for Kubernetes deployment is a practical option. Its fast, scales well, batlle tested also easier to verify and troubleshoot. Unless there are special needs (like support of port ranges) which can be met only with iptables based Kube-proxy, an IPVS based service proxy is better option. Since we are building on top of solid fundation which does heavy lifiting a solution can implemented few hundred lines of code and less error prone.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;github-card&#34; data-github=&#34;cloudnativelabs/kube-router&#34; data-width=&#34;720&#34; data-height=&#34;&#34; data-theme=&#34;default&#34;&gt;&lt;/div&gt;
&lt;script src=&#34;//cdn.jsdelivr.net/github-cards/latest/widget.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Enforcing Kubernetes network policies with iptables</title>
      <link>https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/</link>
      <pubDate>Wed, 03 May 2017 00:00:00 +0000</pubDate>
      <author>info@cloudnativelabs.net (Cloudnative Labs)</author>
      <guid>https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/</guid>
      <description>

&lt;p&gt;Network policies in Kubernetes provides primary means to secure a pod by exerting control over who can connect to pod. Intent of this blog post is not to describe what network policies are but to show how iptables on the the cluster nodes can be used to build a distributed firewall solution that enforces network policies in Kubernetes clusters. This write up draws up from the insights of implementing a network policy controller in &lt;a href=&#34;https://github.com/cloudnativelabs/kube-router&#34;&gt;Kube-router&lt;/a&gt;. But concepts described can be used to build your own version of network policy enforcer with iptables.&lt;/p&gt;

&lt;p&gt;Kubernetes networking has following security model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;for every pod by default ingress is allowed, so a pod can receive traffic from any one&lt;/li&gt;
&lt;li&gt;default allow behaviour can be changed to default deny on per namespace basis. When a namespace is configured with isolation tye of &lt;strong&gt;DefaultDeny&lt;/strong&gt; no traffic is allowed to the pods in that namespace&lt;/li&gt;
&lt;li&gt;when a namespace is configured with &lt;strong&gt;DefaultDeny&lt;/strong&gt; isolation type, network policies can be configured in the namespace to whitelist the traffic to the pods in that namespace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lets quickly recap what network policies are then we will dive in to the iptables based solution aspects.&lt;/p&gt;

&lt;h3 id=&#34;network-policy&#34;&gt;network policy&lt;/h3&gt;

&lt;p&gt;Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDR or IP used for matching source or destination IP&amp;rsquo;s. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of app) and select subsets of objects. A typical network policy looks like below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
   - from:
     - namespaceSelector:
       matchLabels:
         project: myproject
     - podSelector:
       matchLabels:
       role: frontend
   ports:
     - protocol: tcp
     port: 6379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the network policy &lt;strong&gt;matchLabels&lt;/strong&gt; in &lt;strong&gt;podSelector&lt;/strong&gt; section is used to identify set of destination pods for which a network policy applies. Similarly ingress section of network policy has a &lt;strong&gt;matchLabels&lt;/strong&gt; in &lt;strong&gt;podSelector&lt;/strong&gt; section which identifies the set of pods that will become source pods set. Alternatively &lt;strong&gt;namespaceSelector&lt;/strong&gt; in the ingress section can be used to select all pods in a namespace. Ports section of network policy spec defines one or more port and protocol combinations that are to be opened up.&lt;/p&gt;

&lt;h3 id=&#34;evaluating-networking-policies&#34;&gt;evaluating networking policies&lt;/h3&gt;

&lt;p&gt;Network policies are declarative in nature to express application intent. So its implementation of network policies responsibility to translate the intent to configuration. Kubernetes provides a convenient way to retrieve the set of pods matching a label selector through API or kubectl as shown below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;curl 192.168.1.99:8080/api/v1/namespaces/testns1/pods?labelSelector=app%3Dguestbook,tier%3Dfrontend
kubectl get pods -o json --selector=app=guestbook,tier=frontend --namespace=testns1 | jq &#39;.items[] | .status | .podIP&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So its easy to get the set of source pod IP&amp;rsquo;s and set of destination pod IP&amp;rsquo;s based on the &lt;strong&gt;matchLables&lt;/strong&gt; in network policy spec. Essentially a network policy implicitly evaluates to be definition of set of source pods that can talk to set of destination pod over specified port and protocol. Pods being ephermal in nature, set of pods matching the label selector is a dynamic set. In later section we will see how this poses challenges in how delta changes to iptable configuration applied with minimal disruption to datapath. But first lets see how we can configure iptables at a given time to reflect current state of network policies.&lt;/p&gt;

&lt;h3 id=&#34;representing-network-policy-in-iptables&#34;&gt;representing network policy in iptables&lt;/h3&gt;

&lt;p&gt;Since we are only concerned about the filtering of the packets, we will use only iptable filter functionality and all the examples below refer to the use of &lt;strong&gt;filter&lt;/strong&gt; table.&lt;/p&gt;

&lt;p&gt;We have seen how we can evaluate a network policy to a whitelist rules expressing set of source pod IP&amp;rsquo;s that can reach port and protocol on set of destination pod IP&amp;rsquo;s. A natural choice is to represent each network policy as user defined chain. A network policy evaluates to single destination pod set. But a network policy specification can have multiple ingress rules. Each ingress rule evaluates to set of pods that are source pods, port and protocol combination. Below is pseudo code how user chain for the network policy can be populated with rules to accept the traffic.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;for each dstIP in set of destination pod IP as per the podSelector in the network policy {
    for each ingressRule in ingress rules of network policy {
        for each srcIP in set of source pod IP as per the podSelector in $ingressRule {
            for each port, protocol combination in ports section of ingress rule of the network policy {
                 iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT 
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since we have a rule for each source pod ip and destination pod IP combination, as you may have suspected this will blow up the chain with large number of rules. Fortunatley we can use &lt;strong&gt;ipset&lt;/strong&gt; to better represent the rules and more importantly a data path that scales well. A slightly modified pseudo code to populate the rules in the chain.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;form ipset dstIpset with all destination pod ip evaluated as per the podSelector in the policy
for each ingressRule in ingressRules of network policy {
    form ipset srcIpset wth all source pod IP as evaluated as per the podSelector in ingress rule of the network policy 
    for port, protocol in ports of the network policy {
         iptables -A KUBE-NWPLCY-CHAIN -p $protocol -m set --match-set $srcIpset src -m set --match-set dstIpset dst -m $protocol --dport $port -j ACCEPT
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each cluster node can configured with representation of network policies with iptables as above. Destination ipset can be further optimized to represent pod IP of the pods running on specific node. Now lets see how inbound traffic to pod can be run through the network policy chains.&lt;/p&gt;

&lt;h3 id=&#34;pod-ingress-traffic-paths&#34;&gt;Pod ingress traffic paths&lt;/h3&gt;

&lt;p&gt;There are multiple data paths through which packet is received by a pod as shown in below diagram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/pod-ingress-traffic.jpg&#34; alt=&#34;Pod Ingress traffic&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Pod on a node reaching to pod on same node through bridge (pod2-&amp;gt;pod3)&lt;/li&gt;
&lt;li&gt;Pod on a node reaching to pod on same node but through the service proxy (pod1-&amp;gt;pod2)&lt;/li&gt;
&lt;li&gt;Pod on a node reaching to pod on other node (pod3-&amp;gt;pod4)&lt;/li&gt;
&lt;li&gt;Pod on a node reaching to pod on other node through service proxy (pod5-&amp;gt;pod6)&lt;/li&gt;
&lt;li&gt;external client reaching to pod (external client-&amp;gt;pod7)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;pod-specific-firewall-chain&#34;&gt;pod specific firewall chain&lt;/h3&gt;

&lt;p&gt;Ingress traffic to a pod can be whitelisted through one or more network policies. So its important that packet destined to a pod is run through the necessary network policy chains. Again its easier to have iptable chain per pod. Now we need to run through the traffic destined for a pod through rules in the pod specific firewall chain. We only have to consider the pods whose namespace has ingress set to DefaultDeny configuration. Also on a any cluster node, we are only concerned about the pods running on the node.&lt;/p&gt;

&lt;p&gt;For the case #1 data path above we will need to use &lt;strong&gt;physdev&lt;/strong&gt; module to match the traffic that is getting bridged locally. From iptables perspective it will still go through the FORWARD chain. For the case #2 from iptables perspective it is output packet, so it will only hit OUTPUT chain. Rest of the cases #3 to #5 all fall under the same category. Traffic will hit the FORWARD chain. Following is the pseudo code to populate FORWARD and OUTPUT chains to jump the traffic destined for a pod to pod specific firewall chain.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;for each pod running on the node {
    if pod namespace has &#39;DefautDeny&#39; as ingress type {
        iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN
        iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN
        iptables &#39;-A OUTPUT -d $podIP  -j KUBE-POD-SPECIFIC-FW-CHAIN
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;rules-to-run-through-network-policies&#34;&gt;rules to run through network policies&lt;/h3&gt;

&lt;p&gt;We will need rules in the pod specific firewall chain to take appropriate actions. First we need to have default rule as a last rule in the chain to REJECT the traffic in case traffic destined to pod does not match any white list rules in the network policy chains. We will need a rule to permit the return traffic to the pod (for the connections which originated from the pod) using conntrack. A sample set of rules in the pod specific firewall chain would look like below. Number of rules to jump to the network policy chains depends on the how many network policies are applicable to a pod.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN1
iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN2
iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN3
iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;packet-flow&#34;&gt;packet flow&lt;/h3&gt;

&lt;p&gt;Packet flow through the cluster node goes through following path with pod specific firewall and network policy chains in place:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;destination IP is matched in FORWARD and OUTPUT chains of filter table, if destination ip corresponds to a pod running on the node, that needs ingress blocked by default and only permit the traffic as per the network policies, then jump to pod specific firewall chain.&lt;/li&gt;
&lt;li&gt;in the pod specific firewall chain, if the packet belongs to established or related session then allow the traffic, else&lt;/li&gt;
&lt;li&gt;in the pod specific firewall chain run through all network policy chain, if there is any matching rule (matching source ip, port and protocol) in any of the network policy chains then packet is accepted&lt;/li&gt;
&lt;li&gt;if there is no match of rule in any of the network policy chains then packets get dropped.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;state-synchronization&#34;&gt;state synchronization&lt;/h3&gt;

&lt;p&gt;Like mentioned earlier network policy only express the intent. So its implementations responsibility to translate the intent to desired configuration. There are number of events that need reconfiguring the iptable so that configuration reflects the desired intent. For e.g&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;add/delete of pods&lt;/li&gt;
&lt;li&gt;modification to namespace default ingress behaviour&lt;/li&gt;
&lt;li&gt;add/delete of network policies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these events will cause the add/delete of rules in the chains, or add/deletion of network policy and pod specific firewall chain. Implementation of network policies have to watch for the events from Kubernetes API server and have to reflect the changes in the iptables configuration.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;conclusion&lt;/h3&gt;

&lt;p&gt;We went over how iptables can be used to implement a solution that can enforce network policies on each cluster node. An implementation of approach discussed in the article is implemented in [Kube-router][1]. You can deploy Kube-router and see how iptables are used as effective solution to enforce network policies.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;github-card&#34; data-github=&#34;cloudnativelabs/kube-router&#34; data-width=&#34;720&#34; data-height=&#34;&#34; data-theme=&#34;default&#34;&gt;&lt;/div&gt;
&lt;script src=&#34;//cdn.jsdelivr.net/github-cards/latest/widget.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kube-router</title>
      <link>https://cloudnativelabs.github.io/post/2017-04-19-kube-router/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      <author>info@cloudnativelabs.net (Cloudnative Labs)</author>
      <guid>https://cloudnativelabs.github.io/post/2017-04-19-kube-router/</guid>
      <description>

&lt;p&gt;In previous &lt;a href=&#34;https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/&#34;&gt;blog&lt;/a&gt; we went over the Kubernetes service discovery, load balancing and network policies. In this blog we will use Kube-router a distributed load balancer, firewall and router for Kubernetes and demonstrate the Kubernetes networking constructs in action.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;github-card&#34; data-github=&#34;cloudnativelabs/kube-router&#34; data-width=&#34;720&#34; data-height=&#34;&#34; data-theme=&#34;default&#34;&gt;&lt;/div&gt;
&lt;script src=&#34;//cdn.jsdelivr.net/github-cards/latest/widget.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We will setup a Kubernetes cluster from scratch and use kube-router instead of kube-proxy and demonstrate how kube-router provides a solution for cross-node pod-to-pod networking, provides a service proxy on each node and load balances the traffic. We will also see the network policies in action.&lt;/p&gt;

&lt;h3 id=&#34;cluster-setup&#34;&gt;cluster setup&lt;/h3&gt;

&lt;p&gt;We will setup a 3 node kube-node1 (192.168.1.100), kube-node2 (192.168.1.101), kube-node3 (192.168.1.102) as a cluster nodes and use kube-master (192.168.1.99) as Kubernetes master node. Master and nodes are in subnet 192.168.1.0/24. We will use 10.1.0.0/16 ip range for pod ip&amp;rsquo;s with Kube-node1, kube-node2 and kube-node3 using 10.1.1.0/24, 10.1.2.0/24 and 10.1.3.0/24 subnets respectively as shown in below diagram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/demo-setup.jpg&#34; alt=&#34;Demo setup&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On kube-master download the latest stable kubernetes master (1.6.1 at the point of writing this blog) components from below URL&amp;rsquo;s&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;curl -O https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kube-apiserver
curl -O https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kube-controller-manager
curl -O https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will assume etcd is installed on the kube-master and Docker is installed on all the nodes kube-node1, kube-node2, kube-node3.&lt;/p&gt;

&lt;p&gt;On kube-master node provide appropriate etcd details for the &amp;lsquo;–etcd-servers&amp;rsquo; flag when running Kube-apiserver. Run below Kubernetes components on kube-master as below. We will use 172.16.0.0/16 as cluster IP range. As we will see IP&amp;rsquo;s from cluster ip range will be allocated for services.&lt;/p&gt;

&lt;p&gt;Kube-router will not do subnet management and depends on the kube-controller-manager to do subnet allocation for the nodes from cluster CIDR range. So we will launch kube-controller-manager with &lt;code&gt;kube-controller-manager&lt;/code&gt; and &lt;code&gt;\--allocate-node-cidrs&lt;/code&gt; flags. Also inorder to run kube-router in the host namespace when run as daemonset, we will allow kubernetes to privision pods in the host namespace by passing &lt;code&gt;\--allow-privileged=true&lt;/code&gt; falg to kube-apiserver.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;nohup kube-apiserver --etcd-servers=http://127.0.0.1:2379 --insecure-bind-address=192.168.1.99 --service-cluster-ip-range=172.16.0.0/16 --allow-privileged=true &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
nohup kube-controller-manager --master=http://192.168.1.99:8080 --cluster-cidr=10.1.0.0/16 --allocate-node-cidrs=true  &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
nohup kube-scheduler --master=http://192.168.1.99:8080  &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On each of the nodes (kube-node1, kube-node2, kube-node3) download the stable kubelet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;curl -O https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubelet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download the CNI plugins from &lt;code&gt;https://github.com/containernetworking/cni/releases&lt;/code&gt; and extract them in to /opt/cni/bin folder on each of the nodes (kube-node1, kube-node2, kube-node3).&lt;/p&gt;

&lt;p&gt;On each node create CNI conf file at &lt;code&gt;/etc/cni/net.d/10-kuberouter.conf&lt;/code&gt; by running&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or create manually with below content. Kube-router will require to use &lt;code&gt;bridge&lt;/code&gt; CNI plugin, and &lt;code&gt;host-local&lt;/code&gt; plugin for IPAM. Subnet details need not be specified as kube-controller-manager automatically does the subnet allocation for the nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;mynet&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;bridge&amp;quot;,
  &amp;quot;bridge&amp;quot;: &amp;quot;kube-bridge&amp;quot;,
  &amp;quot;isDefaultGateway&amp;quot;: true,
  &amp;quot;ipam&amp;quot;: {
     &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On each node launch kubelet as below. We need to pass on location of CNI plugins for the &lt;code&gt;\--network-plugin-dir&lt;/code&gt; flag, and CNI conf file location for &lt;code&gt;\--cni-conf-dir&lt;/code&gt;. We need to run kubelet with &lt;code&gt;\--allow-privileged&lt;/code&gt; flag set to true so that kube-router can be launched pod in host namespace.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;nohup kubelet --api-servers=http://192.168.1.99:8080  --network-plugin-dir=/opt/cni/bin  --network-plugin=cni --cni-conf-dir=/etc/cni/net.d/ --allow-privileged=true &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, now the cluster is setup almost. We will use Kube-router instead of Kube-proxy to complete the cluster setup. We have two options to run Kube-router.&lt;/p&gt;

&lt;h3 id=&#34;run-kube-router-as-agent-on-each-node&#34;&gt;run Kube-router as agent on each node&lt;/h3&gt;

&lt;p&gt;Build the kube-router from the sources.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;git clone https://github.com/cloudnativelabs/kube-router.git
go build --ldflags &#39;-extldflags &amp;quot;-static&amp;quot;&#39; -o kube-router kube-router.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternativley you can download prebuilt binary:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;wget https://github.com/cloudnativelabs/kube-router/releases/download/v0.0.1/kube-router
chmod +x kube-router
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run kube-router on all nodes as below. We will use kube-router to provide pod-to-pod networking, service proxy and firewall for the pods.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;kube-router --master=http://192.168.1.99:8080/ --run-router=true --run-firewall=true --run-service-proxy=true
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;run-kube-router-as-deamonset&#34;&gt;run Kube-router as deamonset&lt;/h3&gt;

&lt;p&gt;This is quickest way to deploy kube-router. Just run&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-:text&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above will run kube-router as pod on each node automatically.&lt;/p&gt;

&lt;p&gt;Now the cluster is setup with Kube-router providing service proxy, pod ingress firewall and for pod networking. Let see the demo how Kube-router provide these functionality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asciinema.org/a/118056.png&#34; alt=&#34;asciicast&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;conclusion&lt;/h3&gt;

&lt;p&gt;We have seen how Kube-router is used in a Kubernetes deployment to provide cross node pod-to-pod connectivity, how kube-router provides a service proxy on each node and how ingress firewall to the pods are applied in action.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Networking</title>
      <link>https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/</link>
      <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
      <author>info@cloudnativelabs.net (Cloudnative Labs)</author>
      <guid>https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/</guid>
      <description>

&lt;p&gt;This article gives brief overview of fundamnetal networking concepts in Kubernetes.&lt;/p&gt;

&lt;p&gt;First thing one notices with Kubernetes in comparision to other container orchestration platforms is container itself is not a first class construct in Kubernetes. Containers always exists in the context of pod. So first lets understand the basic Kubernetes building block &lt;em&gt;Pod&lt;/em&gt; that consumes network. A pod is a group of one or more containers that are always co-located and co-scheduled, and run in a shared context. The shared context of a pod is a set of Linux namespaces, cgroups etc. Containers within a pod share an IP address and port space, and can find each other via localhost. They can also communicate with each other using standard inter-process communications. Think of it as an application-specific &amp;ldquo;logical host&amp;rdquo; containing one or more application containers which are relatively tightly coupled. Containers in different pods have distinct IP addresses and can not communicate by IPC. Please refer to Pod &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod/&#34;&gt;overview&lt;/a&gt; for further details. As far as networking is concerned pod is the network endpoint. So essentially Kubernetes networking is all about Pods. With that note lets procced.&lt;/p&gt;

&lt;p&gt;Networking functionality in Kubernetes (as of 1.6.1) broadly address below problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How cross-node pod-to-pod connectivity (for east-west traffic) is achieved.&lt;/li&gt;
&lt;li&gt;How the services running in the pods are discovered by other pods, and how pod-to-pod traffic is load balanced when consuming a service.&lt;/li&gt;
&lt;li&gt;How services running in the pod&amp;rsquo;s are exposed for external access from clients outside the cluster (for north-south traffic).&lt;/li&gt;
&lt;li&gt;How with network segmentation, pods are secured by restricting network access to pods&lt;/li&gt;
&lt;li&gt;How high availability, global load balancing etc can be achieved in federated multi-cluster deployments&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article covers first four of above functionalities.&lt;/p&gt;

&lt;h2 id=&#34;cross-node-pod-to-pod-network-connectivity&#34;&gt;Cross node pod-to-pod network connectivity&lt;/h2&gt;

&lt;p&gt;While Kubernetes is opinionated in how containers are deployed and operated, it is very non-prescriptive of how the network should be designed in which pod&amp;rsquo;s are to be run. Kubernetes imposes the following fundamental requirements on any networking implementation (barring any intentional network segmentation policies):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All pods can communicate with all other pods without NAT&lt;/li&gt;
&lt;li&gt;All nodes running pods can communicate with all pods (and vice-versa) without NAT&lt;/li&gt;
&lt;li&gt;IP that a pod sees itself as is the same IP that other pods see it as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the illustration of these requirements let us use a cluster with two cluster nodes. Nodes are in subnet 192.168.1.0/24 and pods use 10.1.0.0/16 subnet, with 10.1.1.0/24 and 10.1.2.0/24 used by node1 and node2 respectively for the pod IP&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/kube-network-requirement.jpg&#34; alt=&#34;Network requirements&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So from above Kubernetes requirements following communication paths must be established by the network.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nodes should be able to talk to all pods. For e.g. 192.168.1.100 should be able to reach 10.1.1.2, 10.1.1.3, 10.1.2.2 and 10.1.2.3 directly (with out NAT)&lt;/li&gt;
&lt;li&gt;a pod should be able to communicate with all nodes. For e.g. pod 10.1.1.2 should be able to reach 192.168.1.100 and 192.168.1.101 without NAT&lt;/li&gt;
&lt;li&gt;a pod should be able to communicate with all pods. For e.g 10.1.1.2 should be able to communicate with 10.1.1.3, 10.1.2.2 and 10.1.2.3 directly (with out NAT)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we will explore these requirements lays foundation for how the services are discovered and exposed. There can be multiple way to design the network that meets Kubernetes networking requirements with varying degree of complexity, flexibility.&lt;/p&gt;

&lt;h3 id=&#34;network-implementation-for-pod-to-pod-network-connectivity&#34;&gt;Network implementation for pod-to-pod network connectivity&lt;/h3&gt;

&lt;p&gt;Kubernetes does not orchestrate setting up the network and offloads the job to the CNI plug-ins. Please refer to the &lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34;&gt;CNI spec&lt;/a&gt; for further details on CNI specification. Below are possible network implementation options through CNI plugins which permits pod-to-pod communication honouring the Kubernetes requirements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;layer 2 (switching) solution&lt;/li&gt;
&lt;li&gt;layer 3 (routing) solution&lt;/li&gt;
&lt;li&gt;overlay solutions&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;layer-2-solution&#34;&gt;layer 2 solution&lt;/h4&gt;

&lt;p&gt;This is the simplest approach should work well for small deployments. Pods and nodes should see subnet used for pod ip&amp;rsquo;s as a single l2 domain. Pod-to-pod communication (on same or across hosts) happens through ARP and L2 switching. We could use &lt;strong&gt;bridge&lt;/strong&gt; CNI plug-in to reuse a L2 bridge for pod containers with below configuration on node1 (note /16 subnet).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; {
     &amp;quot;name&amp;quot;: &amp;quot;mynet&amp;quot;,
     &amp;quot;type&amp;quot;: &amp;quot;bridge&amp;quot;,
     &amp;quot;bridge&amp;quot;: &amp;quot;kube-bridge&amp;quot;,
     &amp;quot;isDefaultGateway&amp;quot;: true,
     &amp;quot;ipam&amp;quot;: {
         &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,
         &amp;quot;subnet&amp;quot;: &amp;quot;10.1.1.0/16&amp;quot;
     }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kube-bridge need to be pre-created such that ARP packets go out on the physical interface. In order for that we have another bridge with physical interface connected to it and node ip assigned to it to which kube-bridge is hooked through the veth pair like below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/l2-network.jpg&#34; alt=&#34;image alt &amp;lt;&amp;gt;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can pass a bridge which is pre-created, in which case &lt;strong&gt;bridge&lt;/strong&gt; CNI plugin will reuse the bridge, only change it would do is to configure gateway for the interface.&lt;/p&gt;

&lt;h4 id=&#34;layer-3-solutions&#34;&gt;layer 3 solutions&lt;/h4&gt;

&lt;p&gt;A more scalable approach is to use node routing than switching the traffic to the pods. We could use bridge CNI plug-in to create a bridge for pod containers with gateway configured. For e.g. on node1 below configuration can be used (note /24 subnet).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; {
     &amp;quot;name&amp;quot;: &amp;quot;mynet&amp;quot;,
     &amp;quot;type&amp;quot;: &amp;quot;bridge&amp;quot;,
     &amp;quot;bridge&amp;quot;: &amp;quot;kube-bridge&amp;quot;,
     &amp;quot;isDefaultGateway&amp;quot;: true,
     &amp;quot;ipam&amp;quot;: {
         &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,
         &amp;quot;subnet&amp;quot;: &amp;quot;10.1.1.0/24&amp;quot;
     }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So how does pod1 with ip 10.1.1.2 running on node1 communicate with pod3 with ip 10.1.2.2 running on node2? we need a way for nodes to route the traffic to other node pod subnets.&lt;/p&gt;

&lt;p&gt;We could populate default gateway router with routes for the subnet as shown in the below diagram. Route to 10.1.1.0/24 and 10.1.2.0/24 is configured to be through node1 and node2 respectively. We could automate keeping the route tables updated as nodes are added or deleted in to the cluster. We can also use some of the container networking solutions which can do the job on public clouds, for e.g. Flannel back end for AWS and GCE, Weaves AWS-VPC mode etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/l3-gateway-routing.jpg&#34; alt=&#34;image alt &amp;lt;&amp;gt;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Alternativley each node can be populated with routes to the other subnets as shown in the below diagram. Again updating the routes can be automated in small/static environment as nodes are added/deleted in the cluster. Or container networking solutions like calico, or Flannel host gateway back end can be used to achieve same.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/l3-host-routing.jpg&#34; alt=&#34;image alt &amp;lt;&amp;gt;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;overlay-solutions&#34;&gt;overlay solutions&lt;/h4&gt;

&lt;p&gt;Unless there is a specific reason an overlay solution for Kubernetes does not make sense considering the networking model of Kubernetes and lack of support for multiple networks. Kubernets requires that nodes should be able to reach pod, even though pods are in overlay network. Similarly pod should be able to reach any node as well. We will need host routes in the nodes set such that pods and nodes can talk to each other. Since inter host pod-to-pod traffic should not be visible in the underlay, we need a virtual/logical network that is overlaid on the underlay. Pod-to-pod traffic need to be encapsulated at the source node. The encapsulated packet is then forwarded to destination node where it is de-encapsulated. A solution can be built around any existing Linux encapsulation mechanisms. We need to have tunnel interface (with VXLAN, GRE etc encapsulation) and host route such that inter node pod-to-pod traffic is routed through the tunnel interface. Below is very generalized view of how a overlay solution can be built that can meet Kubernetes network requirements. Unlike previous solutions there is significant effort in overlay approach setting up tunnels, populating FDB etc. Existing container networking solutions like Weave, Flannel can be used to setup a Kubernetes deployment with overlay networks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/overlay.jpg&#34; alt=&#34;image alt &amp;lt;&amp;gt;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;service-discovery-and-load-balancing&#34;&gt;service discovery and load balancing&lt;/h2&gt;

&lt;p&gt;With the understanding of Kubernetes network requirements of how cross-node pod-to-pod, and node to pod communication works, lets explore the critical functionality of service discovery and load-balancing. Any non-trivial containerized application will end up running multiple pods running different services (a web server, DB server etc). This leads to a problem: how some set of Pods running a service provide functionality to other Pods inside the Kubernetes cluster, how do a service consuming pod find out and keep track of which backend pods are providing a service? Problem is compounded by the fact that pods itself can be ephemeral.&lt;/p&gt;

&lt;h3 id=&#34;services-and-endpoints&#34;&gt;services and endpoints&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt; abstraction in Kubernetes is essential building block that helps in service discovery and load balancing. A Kubernetes Service is an abstraction which defines a logical set of Pods based on labels. Labels are key/value pairs that are attached to objects, such as pods. The label selector is the core grouping primitive in Kubernetes, that can identify a set of objects with matching labels. Kubernetes service leverages label selector to group set of Pods targeted by a Service. Use of labels to group pods to service significantly eases the managing target pool of pods. Traditionally managing pool of endpoints or targets to load balanced service is explicitly done by adding or removing endpoint (for e.g adding instance to AWS ELB, GCE load balancer). Kubernetes implicitly manages endpoitns of service through use of labels.&lt;/p&gt;

&lt;p&gt;Set of pods forming the service is dynamic set, so Kubernetes provides another abstraction of endpoints for the service which gives the list of pods matching the service at the point of query.&lt;/p&gt;

&lt;p&gt;Lets walk through an example. In the below example first we created 2 pods each marked with label &amp;lsquo;app=nginx&amp;rsquo; and expose port 80. We created a service with selector matching labels &amp;lsquo;app=nginx&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/svc1.png&#34; alt=&#34;pod and service selector&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see both the pods selected as endpoints for the service.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloudnativelabs.github.io/img/svc2.png&#34; alt=&#34;service and endpoints&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;exposing-service&#34;&gt;exposing service&lt;/h3&gt;

&lt;p&gt;A pod that want to consume service, can get list of endpoints and do client side loadbalancing to manage how and to which endpoint it connects. But the most common case is server side load balancing where a service endpoints are fronted by virtual ip and load balancer that load balances traffic to virtual ip to endpoints. Idea of load balancing traffic to service endpoints is integrated in to Kubernetes service defintion. Kubernetes allows to specify what kind of service you want. Following are the service types and their behaviors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ClusterIP: A service of ClusterIP type exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster. Cluster internal IP is allocated from the cluster CIDR for the service, and acts as VIP on each node that can be reached by the pods. Cluster IP is not routable, and can be reached only by the pods running on the node.&lt;/li&gt;
&lt;li&gt;NodePort: Exposes the service on each Node&amp;rsquo;s IP at a static port (the NodePort). You&amp;rsquo;ll be able to contact the NodePort service, from outside the cluster, by requesting :.&lt;/li&gt;
&lt;li&gt;LoadBalancer: Exposes the service externally using a cloud provider&amp;rsquo;s load balancer. NodePort and ClusterIP services, to which the external load balancer will route, are automatically created.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both &lt;em&gt;ClusterIP&lt;/em&gt; and &lt;em&gt;NodePort&lt;/em&gt; service types are expected to create service proxy on the node, which can be accessed by the pods running on the node in case of &lt;em&gt;ClusterIP&lt;/em&gt; and accessed from the cluster in case of &lt;em&gt;NodePort&lt;/em&gt;. Service proxy load balances traffic to the appropriate endpoint of the service. Hopefully this explains the network requirements of Kuberntes that we discussed earlier where a pod can reach a node and vice versa.&lt;/p&gt;

&lt;h3 id=&#34;service-discovery&#34;&gt;service discovery&lt;/h3&gt;

&lt;p&gt;Kubernetes API provide all the details about the service, but how does pods learn the details of the service? Kubernetes supports 2 primary modes of finding a Service. When a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service with pre-defined convention on the environment variable names. Other alternative is to use built Kubernetes DNS service which can be added as addon in the cluster. The DNS server watches the Kubernetes API for new Services and creates a set of DNS records for each. If DNS has been enabled throughout the cluster then all Pods should be able to do name resolution of Services automatically.&lt;/p&gt;

&lt;h3 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;/h3&gt;

&lt;p&gt;Kube-proxy is core component of Kubernetes running on each node, that uses iptables to provide a service proxy. Kube-proxy configures iptables such that both Cluster IP and Node ports are available as services on the node for the pods. Traffic is not exactly load balanced but forwarded equally to the endpoints of the service. Kube-proxy provides only L4 load balancing. Kube-proxy itself is not a mandatory component and is replaceable. It just provides out-of-box service proxy solution. If you want L7 load balancing between the services, or use true load balancer like HAproxy, Nginx there are community solutions (for e.g. Linkerd) that are available.&lt;/p&gt;

&lt;h2 id=&#34;ingress-resource-and-ingress-controller&#34;&gt;Ingress resource and Ingress controller&lt;/h2&gt;

&lt;p&gt;While Kubernetes abstraction of services provide a discovery and internal load balancing for the pods with in the cluster, we need a way to expose the service externally to the internet (north-source traffic). An Ingress abstraction in Kubernetes is a collection of rules that allow inbound connections to reach the cluster services. Ingress abstraction only gives a mechanism to define the rules, but you will need an implementation of these rules, known as &amp;lsquo;Ingress Controlles&amp;rsquo;. Kubernetes does not come with a default or out-of-box ingress controller but there are third party solutions like Traefik, Nginx are available as ingress controllers. Ingress controller also provide L7 load balancing unlike cluster services.&lt;/p&gt;

&lt;h2 id=&#34;network-policies&#34;&gt;Network policies&lt;/h2&gt;

&lt;p&gt;We have learned so far how pods can communicate with each other directly or through service proxy. We also learned how pods through services get exposed externally out side the cluster. Now logical question is how do we secure pods? A &lt;em&gt;network policy&lt;/em&gt; in Kubernetes, is a specification of how selections of pods are allowed to communicate with each other and other network endpoints.&lt;/p&gt;

&lt;p&gt;Isolation policies are configured on a per-namespace basis. Once isolation is configured on a namespace it will be applied to all pods in that namespace. Currenlty only &amp;lsquo;DefaultDeny&amp;rsquo; policy is configurable on namespace. Once configured, all the pods in the namepace ingress is blocked. You need to explicitly configure whitelist rules through the network policies. Network policies leverage existing core Kubernetes concepts labels and provide elegant way of expressing application security intents. For e.g below network policy consicley express, intent that only pods with matching label of &amp;lsquo;role: frontend&amp;rsquo; can access the pods with matching label of &amp;lsquo;role: db&amp;rsquo; on port 6379 in that namespace.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
   - from:
     - namespaceSelector:
       matchLabels:
         project: myproject
     - podSelector:
       matchLabels:
       role: frontend
   ports:
     - protocol: tcp
     port: 6379
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have covered how the cross-node pod-to-pod networking works, how services are exposed with in the cluster to the pods, and externally. What makes Kubernetes networking interesting is how the design of core concepts like services, network policy ete permits several possible implementations. Though some core components and addons provide default implementation, they are totally replaceable. There whole ecosystem of network solution that plugs neatly in to Kubernetes networking semantics. In &lt;a href=&#34;http://cloudnativelabs.github.io/blog/post/kube-router/&#34;&gt;kube-router&lt;/a&gt; blog we will walk through a solution for Kubernetes that provides cross-node pod-to-pod networking, service proxy and ingress firewall for the pods.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>